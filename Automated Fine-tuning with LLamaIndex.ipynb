{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LlamaIndex to Automate the fine-tuning of GPT-3.5-turbo on source documents\n",
    "\n",
    "Primarly Extended from [this](https://colab.research.google.com/drive/1vWeJBXdFEObuihO7Z8ui2CAYkdHQORqo?usp=sharing) notebook, we'll take a look at how we can wrap this process into Chainlit and have our own dynamic fine-tuning machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "googleapis-common-protos 1.60.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U llama-index pypdf sentence-transformers ragas openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "openai_api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3322k  100 3322k    0     0   649k      0  0:00:05  0:00:05 --:--:--  830k\n"
     ]
    }
   ],
   "source": [
    "!curl https://jaydixit.com/files/PDFs/TheultimateHitchhikersGuide.pdf --output hitchhikers.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"hitchhikers.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "# Shuffle the documents\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(documents)\n",
    "\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    ")\n",
    "\n",
    "question_gen_query = (\n",
    "    \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "    \"a quiz/examination. Using the provided context from a \"\n",
    "    \"report on climate change and the oceans, formulate \"\n",
    "    \"a single question that captures an important fact from the \"\n",
    "    \"context. Restrict the question to the context information provided.\"\n",
    ")\n",
    "\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents[:50],\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_35_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Questions with `gpt-3.5-turbo`\n",
    "\n",
    "We can use the `generate_questions_from_nodes()` method of our dataset generator to produce a number of questions that will be used to fine-tune!\n",
    "\n",
    "> NOTE: This cell will take ~30s-2min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  40  questions\n"
     ]
    }
   ],
   "source": [
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek and see what was created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What did Zaphod find on the external monitor screens in the Horsehead Nebula?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our questions into a text file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Generator\n",
    "\n",
    "Let's generate questions from a different segment of our documents in order to build a robust test for our RAQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents[\n",
    "        50:\n",
    "    ],  # since we generated ~1 question for 40 documents, we can skip the first 40\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_35_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll use `gpt-3.5-turbo` to generate some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  40  questions\n"
     ]
    }
   ],
   "source": [
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our results for evaluations later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating base `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load up our evaluation questions and get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is constructing our `VectorIndex` so we can move onto testing the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "# limit the context window to 2048 tokens so that refine is used\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3), context_window=2048\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=gpt_35_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we're actually putting the model to the test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tested our model - let's evaluate it to see how it performed!\n",
    "\n",
    "We're testing our model with the `ragas` framework - found [here](https://github.com/explodinggradients/ragas)\n",
    "\n",
    "You'll notice that we're testing two primary metrics:\n",
    "\n",
    "- [`answer_relevancy`](https://github.com/explodinggradients/ragas/blob/a55c3be8b2389501c5c761df9070126027a4d1d6/src/ragas/metrics/answer_relevance.py#L32): This measures how relevant is the generated answer to the prompt. If the generated answer is incomplete or contains redundant information the score will be low. This is quantified by working out the chance of an LLM generating the given question using the generated answer. Values range (0,1), higher the better.\n",
    "- [`faithfulness`](https://github.com/explodinggradients/ragas/blob/a55c3be8b2389501c5c761df9070126027a4d1d6/src/ragas/metrics/faithfulnes.py#L63): This measures the factual consistency of the generated answer against the given context. This is done using a multi step paradigm that includes creation of statements from the generated answer followed by verifying each of these statements against the context. The answer is scaled to (0,1) range. Higher the better.\n",
    "\n",
    "Read more about their implementations [here](https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md)\n",
    "\n",
    "Again, these cells might take some time to complete - be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [01:45<00:00, 35.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [05:29<00:00, 109.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.7865, 'answer_relevancy': 0.9341, 'faithfulness': 0.6792}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eval = {'ragas_score': 0.8230, 'answer_relevancy': 0.9308, 'faithfulness': 0.7375}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging `gpt-4` to improve our `gpt-3.5-turbo` base model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-4\", temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"train_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=gpt_4_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this process will take a few minutes. \n",
    "\n",
    "While this is a powerful technique - it is unfortunately quite slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the fine-tuning dataset\n",
    "\n",
    "Now that we have a number of fine-tuning events from our `OpenAIFineTuningHandler()`, let's save them to a `.jsonl` file - the expected format for fine-tuning `gpt-3.5-turbo`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 51 examples to finetuning_events.jsonl\n"
     ]
    }
   ],
   "source": [
    "finetuning_handler.save_finetuning_events(\"finetuning_events.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "file_response = openai.File.create(file=open(\"finetuning_events.jsonl\", \"rb\"), purpose='fine-tune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-NpALq1Yg4YL2SmFyE2M62xCQ at 0x7f6218bee950> JSON: {\n",
       "  \"object\": \"file\",\n",
       "  \"id\": \"file-NpALq1Yg4YL2SmFyE2M62xCQ\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"filename\": \"file\",\n",
       "  \"bytes\": 218932,\n",
       "  \"created_at\": 1693005279,\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "response = None\n",
    "\n",
    "while not response:\n",
    "  try:\n",
    "    response = openai.FineTuningJob.create(training_file=file_response.id, model=\"gpt-3.5-turbo\")\n",
    "  except:\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-y0mYengpv45M6UBlnJohUsiV at 0x7f622d3c7e00> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-y0mYengpv45M6UBlnJohUsiV\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1693005307,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-JA8wesYIuJRwJseNuF4F5cpO\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"created\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-NpALq1Yg4YL2SmFyE2M62xCQ\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": null\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_id = response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-y0mYengpv45M6UBlnJohUsiV at 0x7f62183cacc0> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-y0mYengpv45M6UBlnJohUsiV\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1693005307,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-JA8wesYIuJRwJseNuF4F5cpO\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"running\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-NpALq1Yg4YL2SmFyE2M62xCQ\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": null\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTuningJob.retrieve(training_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-Vn3flihaJJfPvY9Wpk8gkeaI\",\n",
      "      \"created_at\": 1693005944,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tuning job successfully completed\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-ZXInkPIhAGfEX6zuIdGaXBli\",\n",
      "      \"created_at\": 1693005942,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"New fine-tuned model created: ft:gpt-3.5-turbo-0613:handshake::7raTNuPU\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-QT4yN6Ujp1W92tgiEmIzN2ca\",\n",
      "      \"created_at\": 1693005931,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 150/153: training loss=0.10\",\n",
      "      \"data\": {\n",
      "        \"step\": 150,\n",
      "        \"train_loss\": 0.10006184875965118,\n",
      "        \"train_mean_token_accuracy\": 0.9444444179534912\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-Mhod313tawcewxDQUdhEk0cd\",\n",
      "      \"created_at\": 1693005919,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 140/153: training loss=0.01\",\n",
      "      \"data\": {\n",
      "        \"step\": 140,\n",
      "        \"train_loss\": 0.00531972199678421,\n",
      "        \"train_mean_token_accuracy\": 1.0\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-DkBQfbqIlLJoRXh9bWQaSsaY\",\n",
      "      \"created_at\": 1693005909,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 130/153: training loss=0.09\",\n",
      "      \"data\": {\n",
      "        \"step\": 130,\n",
      "        \"train_loss\": 0.08580644428730011,\n",
      "        \"train_mean_token_accuracy\": 1.0\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-ZKBHBobAPrcCxkIPSbzTl6yH\",\n",
      "      \"created_at\": 1693005897,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 120/153: training loss=0.00\",\n",
      "      \"data\": {\n",
      "        \"step\": 120,\n",
      "        \"train_loss\": 0.0003201231884304434,\n",
      "        \"train_mean_token_accuracy\": 1.0\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-YoBR4sxa2t8gm8DUkNz334BF\",\n",
      "      \"created_at\": 1693005885,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 110/153: training loss=0.09\",\n",
      "      \"data\": {\n",
      "        \"step\": 110,\n",
      "        \"train_loss\": 0.08801983296871185,\n",
      "        \"train_mean_token_accuracy\": 0.9473684430122375\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-mgYFhsNOaa8bSQqq1rXLya4g\",\n",
      "      \"created_at\": 1693005875,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 100/153: training loss=0.19\",\n",
      "      \"data\": {\n",
      "        \"step\": 100,\n",
      "        \"train_loss\": 0.19179511070251465,\n",
      "        \"train_mean_token_accuracy\": 0.9489796161651611\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-3pPaZLvwcLY1jhFkOLv7385C\",\n",
      "      \"created_at\": 1693005863,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 90/153: training loss=0.24\",\n",
      "      \"data\": {\n",
      "        \"step\": 90,\n",
      "        \"train_loss\": 0.23651179671287537,\n",
      "        \"train_mean_token_accuracy\": 0.9222221970558167\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-tHGnzdEpBZY3Vyj1p3IwxZQy\",\n",
      "      \"created_at\": 1693005851,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 80/153: training loss=0.43\",\n",
      "      \"data\": {\n",
      "        \"step\": 80,\n",
      "        \"train_loss\": 0.4335588812828064,\n",
      "        \"train_mean_token_accuracy\": 0.8620689511299133\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_more\": true\n",
      "}\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "while openai.FineTuningJob.retrieve(training_id).status == \"running\":\n",
    "  clear_output(wait=True)\n",
    "  time.sleep(5)\n",
    "  print(openai.FineTuningJob.list_events(id=training_id, limit=10))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-y0mYengpv45M6UBlnJohUsiV at 0x7f61723e4b30> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-y0mYengpv45M6UBlnJohUsiV\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1693005307,\n",
       "  \"finished_at\": 1693005943,\n",
       "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:handshake::7raTNuPU\",\n",
       "  \"organization_id\": \"org-JA8wesYIuJRwJseNuF4F5cpO\",\n",
       "  \"result_files\": [\n",
       "    \"file-TdZTCqBHmSFgmueVr7WZ5yKI\"\n",
       "  ],\n",
       "  \"status\": \"succeeded\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-NpALq1Yg4YL2SmFyE2M62xCQ\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": 175374\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTuningJob.retrieve(training_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_id = openai.FineTuningJob.retrieve(training_id).fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-3.5-turbo-0613:handshake::7raTNuPU\n"
     ]
    }
   ],
   "source": [
    "print(ft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the fine-tuned model\n",
    "\n",
    "Now that we've fine-tuned our model on the `gpt-4` enhanced question answers - let's see how it performs on our `raga` evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "\n",
    "ft_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=ft_model_id, temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=ft_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [01:44<00:00, 34.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [04:18<00:00, 86.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.8811, 'answer_relevancy': 0.9431, 'faithfulness': 0.8267}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_eval = {'ragas_score': 0.8599, 'answer_relevancy': 0.9398, 'faithfulness': 0.7925}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Differences\n",
    "\n",
    "Now we can compare the outputs of the two models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27668/742759091.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'questions' is not defined"
     ]
    }
   ],
   "source": [
    "print(questions[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The bird claimed that reverse engineering enables them to quickly analyze and understand the technology of a spaceship, allowing them to potentially replicate or modify it for their own purposes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(service_context=gpt_35_context)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "\n",
    "ft_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=ft_model_id, temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The bird claimed that reverse engineering allows them to bypass the waiting process for a spaceship to pass through their galactic sector in order to assess if it is willing to provide transportation. Through reverse engineering, they can ascertain that a spaceship will offer them a lift and make a request upon its arrival."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(service_context=ft_context)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model answer_relevancy : 0.9308\n",
      "Fine-tuned model answer_relevancy : 0.9398\n",
      "Improvement answer_relevancy : 0.90%\n",
      "\n",
      "Base model faithfulness : 0.7375\n",
      "Fine-tuned model faithfulness : 0.7925\n",
      "Improvement faithfulness : 5.50%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric_list = [\"answer_relevancy\", \"faithfulness\"]\n",
    "\n",
    "for metric in metric_list:\n",
    "  print(\"Base model\", metric, \":\", base_eval[metric])\n",
    "  print(\"Fine-tuned model\", metric, \":\", ft_eval[metric])\n",
    "  print(f\"Improvement {metric} : {(ft_eval[metric] - base_eval[metric])*100:.2f}%\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
